<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>

    <link href="https://getbootstrap.com/docs/5.3/assets/css/docs.css" rel="stylesheet">
    <title>LearnWell</title>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>

    <style>
        header {
    background-color:rgb(1, 1, 44);
    color: #fff;
    padding: 10px 0;
}

.container {
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.logo img {
    max-width: 100px;
}

.nav li {
    display: inline;
    margin-right: 20px;
}

.search-bar input {
    padding: 5px;
    border: none;
    border-radius: 5px;
    padding: 10px;
}
#sbutton{
    padding: 6px;
}
nav a{
    background-color: aliceblue;
    text-decoration: none;
    padding: 10px;
}

.user-section a {
    color:black;
    text-decoration: none;
    margin-left: 10px;
    background-color: aliceblue;
    padding: 10px;
}
        body{
    background-color: rgb(195, 195, 230);
}
p{
    font-size: medium;
    font-family: 'Times New Roman', Times, serif;
}
img{
    padding: 10px;
}
.flex{
    display: flex;
}
.flex1{
    overflow: hidden;
    padding-left: 100px;
    padding-right: 100px;
    text-align: justify;
}
footer{
    background-color: rgb(1, 1, 44);
}
.fdetails{
    color: #fff;
    font-size: large;
    font-family:'Times New Roman', Times, serif;
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding-left: 100px;
    padding-right: 100px;
}

    </style>
</head>
<body>

    <header>
        <div class="container">
            <div class="logo">
                <a href="learnwell.html"><img src="logo.jpg"></a>
            </div>
            <nav>
                <ul class="nav">
                    <li><a href="art1.html">Space</a></li>
                    <li><a href="art2.html">Nature</a></li>
                    <li><a href="art3.html">Social</a></li>
                    <li><a href="art4.html">Tech</a></li>
                </ul>
            </nav>
            <div class="search-bar">
                <input type="text" placeholder="Search...">
                <button id="sbutton" class="btn btn-primary">Search</button>
            </div>
            <div class="user-section">
                <a href="user.html">Login</a> / <a href="user.html">Register</a>
            </div>
        </div>
    </header>
    <center>
    <img src="Untitled.jpeg">
    <H1>Author : Mariana Lenharo</H1></center>
    <div class="flex">
        <div class="flex1">
    <h2>If AI becomes conscious: here’s how researchers will know</h2>
    <p>Science fiction has long entertained the idea of artificial intelligence becoming conscious — think of HAL 9000, the supercomputer-turned-villain in the 1968 film 2001: A Space Odyssey. With the rapid progress of artificial intelligence (AI), that possibility is becoming less and less fantastical, and has even been acknowledged by leaders in AI. Last year, for instance, Ilya Sutskever, chief scientist at OpenAI, the company behind the chatbot ChatGPT, tweeted that some of the most cutting-edge AI networks might be “slightly conscious”.

        Many researchers say that AI systems aren’t yet at the point of consciousness, but that the pace of AI evolution has got them pondering: how would we know if they were?
        Decades-long bet on consciousness ends — and it’s philosopher 1, neuroscientist 0
        To answer this, a group of 19 neuroscientists, philosophers and computer scientists have come up with a checklist of criteria that, if met, would indicate that a system has a high chance of being conscious. They published their provisional guide earlier this week in the arXiv preprint repository1, ahead of peer review. The authors undertook the effort because “it seemed like there was a real dearth of detailed, empirically grounded, thoughtful discussion of AI consciousness,” says co-author Robert Long, a philosopher at the Center for AI Safety, a research non-profit organization in San Francisco, California.
        The team says that a failure to identify whether an AI system has become conscious has important moral implications. If something has been labelled ‘conscious’, according to co-author Megan Peters, a neuroscientist at the University of California, Irvine, “that changes a lot about how we as human beings feel that entity should be treated”.
        Long adds that, as far as he can tell, not enough effort is being made by the companies building advanced AI systems to evaluate the models for consciousness and make plans for what to do if that happens. “And that’s in spite of the fact that, if you listen to remarks from the heads of leading labs, they do say that AI consciousness or AI sentience is something they wonder about,” he adds.
        Nature reached out to two of the major technology firms involved in advancing AI — Microsoft and Google. A spokesperson for Microsoft said that the company’s development of AI is centred on assisting human productivity in a responsible way, rather than replicating human intelligence. What’s clear since the introduction of GPT-4 — the most advanced version of ChatGPT released publicly — “is that new methodologies are required to assess the capabilities of these AI models as we explore how to achieve the full potential of AI to benefit society as a whole”, the spokesperson said. Google did not respond.
        
        <h3>What is consciousness?</h3>
        One of the challenges in studying consciousness in AI is defining what it means to be conscious. Peters says that for the purposes of the report, the researchers focused on ‘phenomenal consciousness’, otherwise known as the subjective experience. This is the experience of being — what it’s like to be a person, an animal or an AI system (if one of them does turn out to be conscious).
        ChatGPT broke the Turing test — the race is on for new ways to assess AI
        There are many neuroscience-based theories that describe the biological basis of consciousness. But there is no consensus on which is the ‘right’ one. To create their framework, the authors therefore used a range of these theories. The idea is that if an AI system functions in a way that matches aspects of many of these theories, then there is a greater likelihood that it is conscious.
        They argue that this is a better approach for assessing consciousness than simply putting a system through a behavioural test — say, asking ChatGPT whether it is conscious, or challenging it and seeing how it responds. That’s because AI systems have become remarkably good at mimicking humans.
        The group’s approach, which the authors describe as theory-heavy, is a good way to go, according to neuroscientist Anil Seth, director of the centre for consciousness science at the University of Sussex near Brighton, UK. What it highlights, however, “is that we need more precise, well-tested theories of consciousness”, he says.
        
        <h3>A theory-heavy approach</h3>
        To develop their criteria, the authors assumed that consciousness relates to how systems process information, irrespective of what they are made of — be it neurons, computer chips or something else. This approach is called computational functionalism. They also assumed that neuroscience-based theories of consciousness, which are studied through brain scans and other techniques in humans and animals, can be applied to AI.
        <h3>Can lab-grown brains become conscious?</h3>
        On the basis of these assumptions, the team selected six of these theories and extracted from them a list of consciousness indicators. One of them — the global workspace theory — asserts, for example, that humans and other animals use many specialized systems, also called modules, to perform cognitive tasks such as seeing and hearing. These modules work independently, but in parallel, and they share information by integrating into a single system. A person would evaluate whether a particular AI system displays an indicator derived from this theory, Long says, “by looking at the architecture of the system and how the information flows through it”.
        Seth is impressed with the transparency of the team’s proposal. “It’s very thoughtful, it’s not bombastic and it makes its assumptions really clear,” he says. “I disagree with some of the assumptions, but that’s totally fine, because I might well be wrong.”</p>

  
    <label for="comments"><h3>Comments : </h3></label>
    <div>
    <textarea class="form-control" id="comments" placeholder="Enter your comments" rows="6"></textarea>
</div>
</div>
</div>

<footer>
<div class="fdetails" >
    <div>
    <h2>Contacts:</h2>
    <h4>phone no:2357264676</h4>
    <h4>Email:gdfsjs@gmail.com</h4>
    </div>
    <div>
    <h4>Read more on : <a href="art1.html">article1</a> <a href="art2.html">article2</a></h4>
    </div>
    </div>
</footer>
</body>
</html>